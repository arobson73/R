#this is just a list of common commands for R

#number of rows
row(var) - tells the number of rows

#all columns except the first
college = college[,-1]

all columns except specific names
dd = !(names(Auto) %in% c("mpg","mpg_level","name"))
newdata = Auto[,dd]

names(Auto)[!(names(Auto) %in% c("mpg","mpg_level","name"))]   # another way

#read csv
acsv <- read.csv("some.csv")
#read csv strip any unwanted values - good to do at csv read stage then data won't get converted to factors unintentionally
Auto = read.csv("Auto.csv",na.strings=c("?"))

#show csv
fix(acsv)

#repeat
rep(val,number)

#load a source file
source("filename")

#make classification numeric
factor(var) or as.factor(var)

#add new column to data.frame
college=data.frame(college,newcol)

#number of rows or columns of data.frame
nrow(data) or ncol(data)

#histogram
hist(var, breaks=x)

#split plot area into 2 by 2
par(mfrow=c(2,2))

#transpose matrix
t(x)

#clear variables in workspace
rm(list=ls())

#get working directory
getwd()

#set working directory
setwd()

#read table data (assume table is called Auto.data, treat question marks as missing data)
Auto=read.table("Auto.data",header=T, na.strings="?")

#get column names from table
names(Auto)

#show type
class(var)

#apply function over data.frame or list or vector - here range is used, could have used mean or sd (standard deviation for example)
sapply(Auto[,1:7],range)
apply(mat,2,mean)

#remove rows 10 to 85 for example
newAuto = Auto[-(10:85),]

#load library (in this example loading the MASS library
library(MASS)

#get subset - this example gets the subset of chas column that are equal to 1.
subset(Boston,chas == 1)

#median 
median

#uniform random numbers 
runif(100,-1,1)

#linear model fit
lm(y~x)
lm(y~x+I(x^2)) #this is quadratic term also

#plotting on same plot - type is point plot, second plot uses cross hairs
plot(x,y,type='p',col='red')
lines(x,yf1,type='p',pch=4,col='black')

#list files in directory
list.files(".") #this is for current directory

#use a large mark on plot cex
lines(mx,my,type='p',pch=4,cex=4,col='black')

#plot linear regression line fit
plot(c(0,50),c(0,200)).
abline(lm.fit)

#plot linear regression line fit against residuals and more
plot(lm.fit)

#plot scatter plot
pairs(Auto)

#normal distribution
rnorm

#add line to plot
abline(lm.fit,lwd=3,col=2) #using a linear regression fit
abline(-1,0.5,lwd=3,col=3) # using a intercept and slope

#confidence interval 
confint(lm.fit)

#length of data
length

#linear regression fit all
lm(y~.,data=Boston)

#linear regresssion fit all except age
lm(medv~.-age,data=Boston)

#make numeric to classification
Boston$chas = factor(Boston$chas,labels = c("N","Y"))

#correlation
cor(Boston[-c(1,4)]) # correlation ignoring columns 1 and 4

#polynomial regression
lm(crim ~ poly(zn, 3)) #this creates B0 + B1*X + B2*X^2 + B3 *X^3

#logistic regression
glm.fit = glm(dir_dat~Weekly$Lag1+Weekly$Lag2+Weekly$Lag3+Weekly$Lag4+Weekly$Lag5+Weekly$Volume,data=,family=binomial)

#replicate elements of a list
rep("Down",1250)

#compare objects
all.equal(a,b)
identical(a,b)

#create cross table (confusion matrix for example)
table(glm.pred,Direction)

#convert list to character array (flatten a list)
unlist

#convert from data.frame to numeric or character
Weekly["Lag2"][,] # this is same as Weekly$Lag2

#get variable args from function
some_func(...)
{
    dots = list(...)
}

#confusion matrix logisitic regression
train = (Weekly$Year < 2009)
Weekly.0910 = Weekly[!train, ]
glm.fit = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs = predict.glm(glm.fit,newdata= Weekly.0910, type = "response")
glm.pred = rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Up"
Direction.0910 = Weekly$Direction[!train]
conf_mat = table(glm.pred, Direction.0910)
correct = ((conf_mat[1,1] + conf_mat[2,2])/sum(conf_mat)) * 100
error_rate = 100 - correct

#confusion matrix lda
lda.fit = lda(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
lda.pred = predict(lda.fit,Weekly.0910)
lda.class = lda.pred$class
conf_mat_lda = table(lda.class,Direction.0910)
correct_lda = (conf_mat_lda[1,1] + conf_mat_lda[2,2])/sum(conf_mat_lda) * 100
error_rate_lda = 100 - correct_lda

#confusion matrix with knn
library(class)
lag2_train = as.matrix(Weekly$Lag2[train])
lag2_test = as.matrix(Weekly$Lag2[!train])
dir_train = Weekly$Direction[train]
dir_test = Weekly$Direction[!train]
knn.pred = knn(lag2_train,lag2_test,dir_train,k=1)
conf_mat_knn = table(knn.pred,dir_test)


#create matrix
as.matrix()

#error rate
mean(pred == direction)*100

#boxplot
boxplot(displacement~mpg,data=Auto,main="displacement vs mpg")

#for loop
for (i in 1:10000)
    store[i]=i^2

#sample data
sample(1:100,rep=TRUE) # take samples from the 1:100 data with replacement

#install a package
install.packages("scatterplot3d")

#dummy variable
if say some data is "No", "Yes", this can be used in logistic
regression a normal, like    

glm.fit = glm(default~income+balance+student,data = Default,family = binomial)
here the student is dummy variable.

#standard deviation
sd

#confidence interval
t.test(Boston$medv)

#sequence or range of numbers
seq(1,10,by=0.2)

#random seed
set.seed(x)

#available colors
colors()

#lasso
library(glmnet)
grid = 10^seq(10,-2,length = n)
lasso.mod = glmnet(xdat[train,],y[train],alpha=1,lambda=grid)
cv.out = cv.glmnet(xdat[train,],y[train],alpha=1)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod,s=bestlam,newx = xdat[test,])
mean.lasso = mean((lasso.pred-y.test)^2)
out.lasso = glmnet(xdat,y,alpha=1,lambda=grid)
lasso.coef = predict(out.lasso,type="coefficients",s=bestlam)[1:11,]

#another lasso
lasso.fit = cv.glmnet(x_train,y_train,alpha=1,lambda=grid,thresh=1e-12)
bestlam1 = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit,s=bestlam1,newx = x_test)
mean_sq_lasso = mean((lasso.pred-y_test)^2)
x_full = model.matrix(Apps~.,data=college[,-1])
y_full = college$Apps
lasso.full.fit = glmnet(x_full,y_full,alpha=0,lambda=grid,thresh=1e-12)
lasso.coef = predict(lasso.full.fit,type="coefficients",s=bestlam1)



#matrix of data like data frame
xdat = model.matrix(y~.,fr)[,-1] # 

#best subset

library(leaps)
y.fit = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10)
res = summary(y.fit)
c = coef(y.fit,10)
a1 = which.min(res$cp)
a2 = which.min(res$bic)
a3 = which.max(res$adjr2)

#forward step selection
y.fit.fwd = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10,method="forward")
res.fwd = summary(y.fit.fwd)
c.fwd = coef(y.fit.fwd,10)
a1.fwd = which.min(res.fwd$cp)
a2.fwd = which.min(res.fwd$bic)
a3.fwd = which.max(res.fwd$adjr2)


#backward step selection
y.fit.bck = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10,method="backward")
res.bck = summary(y.fit.bck)
c.bck = coef(y.fit.bck,10)
a1.bck = which.min(res.bck$cp)
a2.bck = which.min(res.bck$bic)
a3.bck = which.max(res.bck$adjr2)

#split data into test and training
train = sample(length(dat), length(dat)/2)
test = (-train)
train_dat = dat[train]
test_dat = dat[test]

#mean sq error
mean_sq_lm = mean((pred - college_test$Apps)^2)

#ridge regression
library(glmnet)
x_train = model.matrix(Apps~.,data=college_train[,-1])
y_train = college_train$Apps
x_test = model.matrix(Apps~.,data=college_test[,-1])
y_test = college_test$Apps
grid = 10^seq(10,-2,length = 100)
ridge.fit = cv.glmnet(x_train,y_train,alpha=0,lambda=grid,thresh=1e-12)
bestlam = ridge.fit$lambda.min
ridge.pred = predict(ridge.fit,s=bestlam,newx = x_test)
mean_sq_ridge = mean((ridge.pred-y_test)^2)

#another linear model fit
#fit a linear model
dft = college_train[,-1]
lm.fit = lm(Apps~.,data=dft)
pred = predict(lm.fit,college_test[,-1])
#get mean sq error
mean_sq_lm = mean((pred - college_test$Apps)^2)

#now try the PCR
library(pls)
pcr.fit = pcr(Apps~.,data=college[,-1],subset=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type="MSEP") # this gives  about 10 good choice
pcr.pred = predict(pcr.fit,college_test[,-1],ncomp=17)
mean_sq_pcr = mean((pcr.pred - y_test)^2)

#get R2 (R squared) from the data
mx = mean(test_dat)
mx_sq = ((test_dat- mx)^2)
R2_lm = ((test_dat - lm.pred)^2) / mx_sq

#bar plot
barplot(c(R2_lm,R2_ridge,R2_lasso,R2_PCR,R2_PLS),col="blue",names.arg=c("R2_lm","R2_ridge","R2_lasso","R2_PCR","R2_PLS"), main="Test R sq")
 
#create data set (n by p)
n=1000
p=20
x = rnorm(n,0,1)
e = rnorm(n,0,0.01)
xem = as.matrix(x) # 1 by 1000
B = rnorm(p) #20 by 1 
B[4] = 0
B[11] = 0
B[14]=0
B[19]=0

y = xem %*% B + e # matrix multiply

# column names
x_xcols = colnames(x,do.NULL = FALSE,prefix = "x.")

#get training MSE for subsets
val.errors =rep(NA,p)
x_cols = colnames(xem,do.NULL = FALSE,prefix = "X")
for(i in 1:p)
{
    coefi = coef(fit.sub,id=i)
  #  pred = as.matrix(train_set_x[,x_cols %in% names(coefi)])
    pred = as.matrix(train_set_x[,x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in% x_cols]
    print(pred)
    val.errors[i] = mean((train_set_y - pred)^2)
}
plot(val.errors,ylab= "Training MSE", pch=19,type="b")

#get index at which max or min
which.min(data)
which.max(data)

#sample data into k folds
folds = sample(rep(1:k,length=100))
for(i = 1:k)
 {
     dat = Boston[folds != i,]
 }

#predict for regsubsets with call below
predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    print(coefi)
#this is the returned value
   # print(dim(mat[,names(coefi)] %*% coefi))
#note mat here can be 50 by n. coefi is n by 1 (can be written as 1 by n)
#result is 50 by 1
    mat[, names(coefi)] %*% coefi
}
pred = predict(fit_sub,Boston[folds == i,],id = j)

# model matrix - can expand factors to dummy variables. here we remove the first column (everything except crim)
model.matrix(crim~.-1,data=Boston)

#another lasso with mse rmse
library(glmnet)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.lasso = cv.glmnet(x, y, type.measure = "mse")
plot(cv.lasso)
#perform the rmse with the lasso
#get rmse
rmse.lasso = sqrt(cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se])

#fit data with poly and use k fold cross validation
library(ISLR)
set.seed(3)
p=10
cv.error = rep(NA,10)
for (i in 1:p)
{
    glm.fit = glm(wage~poly(age,i), data= Wage)
    cv.error[i] = cv.glm(Wage,glm.fit,K=p)$delta[1]
}

plot(1:p,cv.error,xlab="degree",ylab="CV error",type="l",pch=20,lwd=2,ylim=c(1590,1700))
min.point = min(cv.error)
sd.points = sd(cv.error)
abline(h=min.point+0.2 * sd.points, col="red",lty="dashed")
abline(h=min.point-0.2 * sd.points, col="blue",lty="dashed")

#nice plot showing predict fit with standard error bands
agelims = range(Wage$age)
age.grid = seq(from=agelims[1],to=agelims[2])
lm.fit = lm(wage~poly(age,3),data=Wage)
preds = predict(lm.fit,newdata=list(age=age.grid),se=TRUE)
se.bands = cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
par(mfrow=c(1,1),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(Wage$age,Wage$wage,xlim=agelims,cex=0.5,col="darkgrey")
title("degree 3 poly",outer=T)
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="red",lty=3)

#predict value using binomial (value > 250)  
fit.step = glm(I(wage > 250)~poly(age,3),data=Wage,family=binomial)
preds.step = predict(fit.step,newdata=list(age=age.grid),se=T)
pfit = exp(preds.step$fit) / (1 + exp(preds.step$fit))
se.bands.logit = cbind(preds.step$fit+2*preds.step$se.fit, preds.step$fit-2*preds.step$se.fit)
se.bands = exp(se.bands.logit)/(1+ exp(se.bands.logit))

plot(Wage$age,I(Wage$wage > 250), xlim=agelims,type="n", ylim=c(0,.2))
points(jitter(Wage$age), I((Wage$wage > 250) / 5), cex= 0.5,pch="|", col="darkgrey")
lines(age.grid,pfit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="red",lty=3)

#cut - split / section / part data into regions / groups
table(cut(Wage$age,4))

#fit a step function
#use a step function use k fold cross validation to get best number of cut points
error.steps = rep(NA,9)
for (i in 2:10)
{
    Wage$age.cut = cut(Wage$age,i)
    lm.fit = glm(wage~age.cut,data=Wage)
    error.steps[i-1] = cv.glm(Wage,lm.fit,K=10)$delta[2]
}
plot(2:10,error.steps,xlab="Number of cuts",ylab="CV error",type="l",pch=20,lwd=2)

min_step = which.min(error.steps) + 1
lm.step = glm(wage~cut(age,min_step),data=Wage)
lm.step.pred = predict(lm.step,data.frame(age=age.grid))
plot(wage~age,data=Wage,col="darkgrey")
lines(age.grid,lm.step.pred,col="red",lwd=2)


#using gam with predict
gam6 = gam(wage~year+s(age,5) +  education,data=Wage)
par(mfrow=c(1,3))
plot(gam6,se=TRUE,col = "blue")
#test F
ares = anova(gam1,gam2,gam3,gam4,gam5,gam6)
#try predict
#using preds on the test data
preds = predict(gam6,newdata=Wage)
npreds = as.numeric(preds)
yt = Wage$wage
mse = mean((npreds - yt)^2)
plot(1:length(yt),yt,type='p',col="blue")
lines(1:length(yt),npreds,type='p',col="red")

#residuals sum of squares
all.rss = rep(NA,10)
for(i in 1:10)
{
    lm.fit = lm(nox~poly(dis,i), data= Boston)
    all.rss[i] = sum((lm.fit$residuals)^2)
}
plot(all.rss)

#get R squared from a gam fit
#fit a gam
library(gam)
gam.fit = gam(Outstate~ Private + s(Room.Board,df=2)+s(PhD,df=2)+s(perc.alumni,df=2)+s(Expend,df=2)+s(Grad.Rate,df=2),data=data_train)
par(mfrow=c(2,3))
plot(gam.fit,se=T,col="blue")

#get test R squared
gam.pred = predict(gam.fit,data_test)
gam.err = mean((data_test$Outstate - gam.pred)^2)

gam.tss = mean((data_test$Outstate - mean(data_test$Outstate))^2)
test.rss = 1 - gam.err/gam.tss

#max of vectors
pmax(c(1,2,3),c(1,0,5))

#3d plot with different views
par(mar=c(1,1,1,1))
layout(matrix(1:4, nrow=2))
s=lapply(c(0,30,60,90), function(t) persp(x,y,z, col='blue', theta=t))

#tree method to predict
set.seed(5)
library(ISLR)
library(MASS)
library(tree)
train = sample(1:nrow(Carseats),nrow(Carseats)/2)
test = -train
tree.carseats = tree(Sales~.-Sales,Carseats[train,])
pred = predict(tree.carseats,newdata=Carseats[test,])
plot(pred,Carseats[test,"Sales"])
abline(0,1)
mse = mean((pred-Carseats[test,"Sales"])^2)
#use cross validation
cv.car = cv.tree(tree.carseats)
plot(cv.car$size,cv.car$dev,type='b')
#from graph seems like 10 is best
prune.car = prune.tree(tree.carseats,best=10)
#now get improved mse
pred_pruned = predict(prune.car,newdata=Carseats[test,])
plot(pred_pruned,Carseats[test,"Sales"])
abline(0,1)
mse_pruned = mean((pred_pruned - Carseats[test,"Sales"])^2)

#now use bagging approach n = p
library(randomForest)
bag.carseats = randomForest(Sales~.-Sales,data=Carseats,subset=train,mtry=ncol(Carseats)-1,importance=TRUE)
pred.bag = predict(bag.carseats,newdata=Carseats[test,])
plot(pred.bag,Carseats[test,"Sales"])
abline(0,1)
mse_bag = mean((pred.bag - Carseats[test,"Sales"])^2)
imp = importance(bag.carseats)

#now try a random forest
ps = (ncol(Carseats)-1)/3

rf.carseats = randomForest(Sales~.,data=Carseats,subset=train,mtry=ps,importance=TRUE)
rf.pred = predict(rf.carseats,newdata=Carseats[test,])
mse_rf = mean((rf.pred - Carseats[test,"Sales"])^2) 

#tree and pruned tree with class type
 #fit tree to OJ with Purchase as the response
tree.OJ = tree(Purchase~.,OJ_train)
plot(tree.OJ)
text(tree.OJ,pretty=0)

pred.OJ = predict(tree.OJ,OJ_test,type="class")
tab.OJ = table(pred.OJ,OJ_test$Purchase)

err.OJ =  (tab.OJ[1,2] + tab.OJ[2,1])  / sum(tab.OJ)
cv.OJ = cv.tree(tree.OJ)
plot(cv.OJ$size,cv.OJ$dev,type='b') # 5 is min
#now produce pruned tree
prune.OJ = prune.tree(tree.OJ,best=5)

#compare test error rates
pred.OJ.pruned = predict(prune.OJ,OJ_test, type="class")
tab.OJ.pruned = table(pred.OJ.pruned,OJ_test$Purchase)
err.OJ.pruned = (tab.OJ.pruned[1,2] + tab.OJ.pruned[2,1]) / sum(tab.OJ.pruned)

#remove column by name in data frame
Hitters[,! names(Hitters) %in% "Salary"]

#remove NA from rows
na.rows = apply(Hitters,1,function(x){any(is.na(x))})
Hitters_new = Hitters[!na.rows,]
Hitters_na = Hitters[na.rows,]

#another way, remove ? from rows
x = rowSums(Auto == "?",na.rm = TRUE) > 0L

#factor level contains unwanted value, remove unwanted level
#note use this after running rowSums (if more levels than levels in the data, it drops extra levels)
x = droplevels(x)

#lasso with glmnet
#now use glmnet - generalised linear model with lasso
library(glmnet)
x = model.matrix(log_sal~.,data = Hitter_log[train,])
y = Hitter_log[train,"log_sal"]
x_test = model.matrix(log_sal~., data=Hitter_log[test,])
lasso.fit = glmnet(x,y,alpha=1)
lasso.pred = predict(lasso.fit, s = 0.01,newx = x_test)
error_lasso = mean((lasso.pred - Hitter_log[test,"log_sal"])^2)

#perform boosting
library(gbm)
lam = seq(0,0.1,0.01)
i=1;
err_test = rep(NA,length(lam))
err_train = rep(NA,length(lam))
for (sh in lam)
{
    boost.boston = gbm(log_sal~.,data=Hitter_log[train,],distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=sh,verbose=F)
pred.train = predict(boost.boston,newdata=Hitter_log[train,],n.trees=1000)
pred.test = predict(boost.boston,newdata=Hitter_log[test,],n.trees=1000)
    err_test[i] = mean((pred.test - Hitter_log[test,"log_sal"])^2)
    err_train[i] = mean((pred.train - Hitter_log[train,"log_sal"])^2)
    i = i+1
    print(i)
}

#convert qualative value to 1 or 0
ifelse(Caravan$Purchase == "Yes",1,0)

#use all except
library(gbm)
boost.cara = gbm(Purchase~.-(PVRAAUT+AVRAAUT),data=Cara_train,distribution="bernoulli",n.trees=1000,shrinkage=0.01)

#correlate specific var
cor(Caravan,Caravan$Purchase)

#place text in plot
text(c(0),c(20), "hello",col="red")

#plot circle
radius = 2
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), asp = 1, xlab = "X1", 
    ylab = "X2")
symbols(c(-1), c(2), circles = c(radius), add = TRUE, inches = FALSE)
text(c(-1), c(2), "< 4")
text(c(-4), c(2), "> 4")

#another circle
#investigate a non linear boundary - this is circle
#(x - h)^2  + (y - k)^2  = r^2
r = 2;
c1 = -1
c2 = 2
p = 100
pis = seq(0,2*pi,pi/p)

x = r* cos(pis) +c1
y = r * sin(pis) + c2

plot(x,y,xlim=c(-5,2),ylim=c(-1,5))

#2 set of vectors choose ones not in the other set
setdiff(1:10,c=(2,4,6)) # picks 1,3,5,7,8,9,10

#svm linear polynomial and radial
#create quadratic
x = rnorm(100)
y = 2*x^2 + 3 + rnorm(100)
type1 = sample(100,50)
type2 = -type1
y[type1] = y[type1] + 3
y[type2] = y[type2] - 3
plot(x[type1],y[type1], col='red', ylim=c(-2,19) )
points(x[type2],y[type2],col='blue')

z = rep(0,100)
z[type1] = 1 # this class is 1
#take 25 from each type to make train and test data
train = c(sample(type1,25), sample(setdiff(1:100,type1),25))
data.train = data.frame(x=x[train],y=y[train],z=as.factor(z[train]))
data.test = data.frame(x=x[-train],y=y[-train],z=as.factor(z[-train]))

library(e1071)
#use a linear support vector classifier
svm.linear = svm(z~.,data=data.train, kernel="linear",cost=10,scale=FALSE)
plot(svm.linear,data.train)

#support vectors are plotted as crosses
#svm.linear$index

pred_train = predict(svm.linear,data.train)
tb = table(z[train],pred_train)

#now try a polynomial kernel
    
svm.poly = svm(z~.,data=data.train, kernel="polynomial",cost=10,scale=FALSE)

plot(svm.poly,data.train)

pred_poly_train = predict(svm.poly,data.train)
tb_poly = table(z[train], pred_poly_train)# no training errors this time

#now try a radial 
svm.rad = svm(z~.,data=data.train, kernel="radial",gamma=1,cost=10,scale=FALSE)

plot(svm.rad,data.train)

pred_rad_train = predict(svm.rad,data.train)
tb_rad = table(z[train], pred_rad_train)# no training errors this time

#now look at the test errors
plot(svm.linear,data.test)
test_lin = table(z[-train],predict(svm.linear,data.test))

test_poly = table(z[-train],predict(svm.poly,data.test))

test_rad = table(z[-train],predict(svm.rad,data.test))

set.seed(421)
n=500
p=2
x1 = runif(500) - 0.5 # mean 0
x2 = runif(500) - 0.5 # mean 0
y = 1*(((x1^2) - (x2^2)) > 0)

#plot the results
plot(x1[y==0],x2[y==0],col='blue',main="initial")
points(x1[y==1],x2[y==1],col='red')

#fit a logisitc regression
dat = data.frame(y,x1,x2)
glm.fit = glm(y~x1+x2,data=dat,family=binomial)
lm.prob = predict(glm.fit,dat,type="response") # here we use training data to predict
lm.pred = ifelse(lm.prob > 0.53,1,0)  #note we just took mean of lm.prob to get 0.53
data.pos = dat[lm.pred==1,]
data.neg = dat[lm.pred==0,]

plot(data.pos$x1,data.pos$x2,col='blue',main="linear")
points(data.neg$x1,data.neg$x2,col='red')

#if we stuck with 0.5 then all would have been classed same value

#now fit logistic regression using non linear functions

glm.fit.nl = glm(y~poly(x1,2)+I(x2),data=dat,family=binomial)
lm.prob.nl = predict(glm.fit.nl,dat,type="response")
lm.pred.nl = ifelse(lm.prob.nl > 0.5,1,0)
data.pos.nl = dat[lm.pred.nl==1,]
data.neg.nl = dat[lm.pred.nl==0,]

plot(data.pos.nl$x1,data.pos.nl$x2,col='blue',main="non linear")
points(data.neg.nl$x1,data.neg.nl$x2,col='red')

#fit a support vector svm
library(e1071)
svm.radial = svm(as.factor(y)~x1+x2,data=dat,kernel="radial",gamma=1,cost=10,scale=FALSE)
plot(svm.linear,dat,main="svm radial")
svm.pred = predict(svm.radial,dat)
data.pos.svm = dat[svm.pred ==1,]
data.neg.svm = dat[svm.pred ==0,]
plot(data.pos.svm$x1,data.pos.svm$x2,col='blue',main="new svm radial")
points(data.neg.svm$x1,data.neg.svm$x2,col='red')

#another svm with tuning tune
#generate  2 classes p=2 so classes are
#just barely separable
set.seed(4)
n=1000
x = matrix(rnorm(n*2), ncol=2)
y = c(rep(-1,n/2),rep(1,n/2))
x[y==1,] = x[y==1,] +  3  
dat = data.frame(y,x)
plot(x[y==-1],col='blue',ylim=c(-5,10))
points(x[y==1],col='red')
library(e1071)
tune.out = tune(svm,as.factor(y)~.,data=dat,kernel="linear",ranges=list(cost=c(0.001,0.01,0.1,1,5,10,100)))
sm = summary(tune.out) # seems like 0.1 is best
smbest = tune.out$best.model

res = data.frame(cost = tune.out$performances$cost, misclass = tune.out$performances$error * 100)   
#try training test

#svm.fit1 = svm(as.factor(y)~.,data=dat,kernel="linear",cost=0.01,scale=FALSE)
#plot(svm.fit1,dat)
costs=c(0.001,0.01,0.1,1,5,10,100)
#create some test data
xt = matrix(rnorm(n*2),ncol=2)
yt = c(rep(-1,n/2),rep(1,n/2))
xt[y==1,] = x[y==1,] + 3
dat_test = data.frame(yt,xt)
errs = rep(NA,length(costs))
errs_test = rep(NA,length(costs))
i=1
for(c in costs)
{
    svm.fit = svm(as.factor(y)~.,data=dat,kernel="linear",cost=c,scale=FALSE)
    pred = predict(svm.fit,dat)
    pred_test = predict(svm.fit,dat_test)
   # print(pred)
    #tabr = table(y,pred)
    errs[i] = sum(pred != dat$y)
    errs_test[i] = sum(pred_test != dat_test$yt)
    i= i + 1
}
#plot pairs plot with formula, plot svm 
plotpairs = function(fit,type) {
    for(name in names(Auto)[!(names(Auto) %in% c("mpg","mpg_level","name"))]){
        plot(fit,Auto,as.formula(paste("mpg~",name,sep="")),main=type)
    }
}

#another plot svm
plot(svm.fit,data,x1~x2) # here just plot the pairs x1 and x2

#simple example of grouping cluster of values
set.seed(5)
X = cbind(c(1,1,0,5,6,4),c(4,3,4,1,2,0))

labels = sample(2,nrow(X),replace=T)

c1 = c(mean(X[labels==1,1]),mean(X[labels==1,2]))
c2 = c(mean(X[labels==2,1]),mean(X[labels==2,2]))


euclid = function(a,b) {
    return (sqrt(c(a[1]-b[1])^2 + (a[2]-b[2])^2)) 
}
labs = function(x,c_1,c_2)
{
    c1 = c_1
    c2= c_2
    print(c1)
    print(c2)
    labs = rep(NA,nrow(x)) 
    for (j in 1:10)
    {
            for(i in 1:nrow(x))
            {
                if( euclid(x[i,],c1) < euclid(x[i,],c2))
                {
                    labs[i] = 1
                }
                else
                {
                    labs[i] = 2
                }
            }

        #update centroids
        c1 = c(mean(X[labs==1,1]),mean(X[labs==1,2]))
        c2 = c(mean(X[labs==2,1]),mean(X[labs==2,2]))
        print(labs)
        print(c1)
        print(c2)    
    }

    
    return (labs)
}

labels_n = labs(X,c1,c2)

plot(X[,1],X[,2], col=(labels_n+1),pch=4,cex=2)
points(c1[1],c1[2],col="blue",pch=3)
points(c2[1],c2[2],col="black",pch=3)

#print multiple variables to the console
cat(sprintf("i=%d,j=%d\n",i,j))

#multiply each row of matrix by vec (which is same length as each row vec)
t1 = t(m)* vec
#sum each row of matrix
t1 = apply(t(t1),1,sum)

#perform routines / functions on matrix
A = sweep(dat,MARGIN=2,10,"*") # multiply by 10 across rows 


#centre and scale variables
scale
or
dmean = apply(USArrests,2,mean)
dsdev = sqrt(apply(USArrests,2,var))
dsc = sweep(USArrests,MARGIN=2,dmean,"-")
dsc = sweep(dsc,MARGIN=2,dsdev,"/")

#principal component

pr.out = prcomp(USArrests, scale=TRUE)
pve = pr.out$sdev^2 / sum(pr.out$sdev^2)
loads = pr.out$rotation # these are the factors 

#hierarchial clustering
hr.complete = hclust(dist(USArrests),method="complete")
plot(hr.complete)
c = cutree(hr.complete,3) # cut into 3 clusters
t1 = table(c)

hr.complete_sc = hclust(dist(scale(USArrests)),method="complete")
c1 = cutree(hr.complete_sc,3)
t2 = table(c1)


#kmeans clustering and principal component
set.seed(6)

dat = matrix(rnorm(20*3*50,mean=0,sd=0.001),ncol=50)

dat[1:20,2] =10 
dat[21:40,1]=2
dat[21:40,2]=2
dat[41:60,1] =10 

labs = c(rep(1,20),rep(2,20),rep(3,20)) 

Cols = function(vec)
{
    cols = rainbow(length(vec))
    return (cols[as.numeric(as.factor(vec))])
}

pr.out = prcomp(dat)
plot(pr.out$x[,1:2],col=1:3,pch=19)

#kmeans cluster
km.out = kmeans(dat,3,nstart=20)
tb = table(km.out$cluster,labs)

#try k =2
km.out2 = kmeans(dat,2,nstart=20)
tb2 = table(km.out2$cluster,labs)

#try k =4
km.out4 = kmeans(dat,4,nstart=20)
tb4 = table(km.out4$cluster,labs)

#use the pr out data

km.pr = kmeans(pr.out$x[,1:2],3,nstart=20)

tbpr = table(km.pr$cluster,labs)

#note the labels are defined randomly here. The kmeans
decides which label, so maybe have to adapt to that
e.g instead of 1,2,3 might need to do 3,2,1 or 1,3,2.
hence we should create label after keans result. 
we know our principal component has identified 3 
regions here.
