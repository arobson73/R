#this is just a list of common commands for R

#number of rows
row(var) - tells the number of rows

#all columns except the first
college = college[,-1]

#read csv
acsv <- read.csv("some.csv")

#show csv
fix(acsv)

#repeat
rep(val,number)

#load a source file
source("filename")

#make classification numeric
factor(var) or as.factor(var)

#add new column to data.frame
college=data.frame(college,newcol)

#number of rows or columns of data.frame
nrow(data) or ncol(data)

#histogram
hist(var, breaks=x)

#split plot area into 2 by 2
par(mfrow=c(2,2))

#transpose matrix
t(x)

#clear variables in workspace
rm(list=ls())

#get working directory
getwd()

#set working directory
setwd()

#read table data (assume table is called Auto.data, treat question marks as missing data)
Auto=read.table("Auto.data",header=T, na.strings="?")

#get column names from table
names(Auto)

#show type
class(var)

#apply function over data.frame or list or vector - here range is used, could have used mean or sd (standard deviation for example)
sapply(Auto[,1:7],range)

#remove rows 10 to 85 for example
newAuto = Auto[-(10:85),]

#load library (in this example loading the MASS library
library(MASS)

#get subset - this example gets the subset of chas column that are equal to 1.
subset(Boston,chas == 1)

#median 
median

#uniform random numbers 
runif(100,-1,1)

#linear model fit
lm(y~x)
lm(y~x+I(x^2)) #this is quadratic term also

#plotting on same plot - type is point plot, second plot uses cross hairs
plot(x,y,type='p',col='red')
lines(x,yf1,type='p',pch=4,col='black')

#list files in directory
list.files(".") #this is for current directory

#use a large mark on plot cex
lines(mx,my,type='p',pch=4,cex=4,col='black')

#plot linear regression line fit
plot(c(0,50),c(0,200)).
abline(lm.fit)

#plot linear regression line fit against residuals and more
plot(lm.fit)

#plot scatter plot
pairs(Auto)

#normal distribution
rnorm

#add line to plot
abline(lm.fit,lwd=3,col=2) #using a linear regression fit
abline(-1,0.5,lwd=3,col=3) # using a intercept and slope

#confidence interval 
confint(lm.fit)

#length of data
length

#linear regression fit all
lm(y~.,data=Boston)

#linear regresssion fit all except age
lm(medv~.-age,data=Boston)

#make numeric to classification
Boston$chas = factor(Boston$chas,labels = c("N","Y"))

#correlation
cor(Boston[-c(1,4)]) # correlation ignoring columns 1 and 4

#polynomial regression
lm(crim ~ poly(zn, 3)) #this creates B0 + B1*X + B2*X^2 + B3 *X^3

#logistic regression
glm.fit = glm(dir_dat~Weekly$Lag1+Weekly$Lag2+Weekly$Lag3+Weekly$Lag4+Weekly$Lag5+Weekly$Volume,family=binomial)

#replicate elements of a list
rep("Down",1250)

#compare objects
all.equal(a,b)
identical(a,b)

#create cross table (confusion matrix for example)
table(glm.pred,Direction)

#convert list to character array (flatten a list)
unlist

#convert from data.frame to numeric or character
Weekly["Lag2"][,] # this is same as Weekly$Lag2

#get variable args from function
some_func(...)
{
    dots = list(...)
}

#confusion matrix logisitic regression
train = (Weekly$Year < 2009)
Weekly.0910 = Weekly[!train, ]
glm.fit = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs = predict.glm(glm.fit,newdata= Weekly.0910, type = "response")
glm.pred = rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Up"
Direction.0910 = Weekly$Direction[!train]
conf_mat = table(glm.pred, Direction.0910)
correct = ((conf_mat[1,1] + conf_mat[2,2])/sum(conf_mat)) * 100
error_rate = 100 - correct

#confusion matrix lda
lda.fit = lda(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
lda.pred = predict(lda.fit,Weekly.0910)
lda.class = lda.pred$class
conf_mat_lda = table(lda.class,Direction.0910)
correct_lda = (conf_mat_lda[1,1] + conf_mat_lda[2,2])/sum(conf_mat_lda) * 100
error_rate_lda = 100 - correct_lda

#confusion matrix with knn
lag2_train = as.matrix(Weekly$Lag2[train])
lag2_test = as.matrix(Weekly$Lag2[!train])
dir_train = Weekly$Direction[train]
dir_test = Weekly$Direction[!train]
knn.pred = knn(lag2_train,lag2_test,dir_train,k=1)
conf_mat_knn = table(knn.pred,dir_test)


#create matrix
as.matrix()

#error rate
mean(pred == direction)*100

#boxplot
boxplot(displacement~mpg,data=Auto,main="displacement vs mpg")

#for loop
for (i in 1:10000)
    store[i]=i^2

#sample data
sample(1:100,rep=TRUE) # take samples from the 1:100 data with replacement

#install a package
install.packages("scatterplot3d")

#dummy variable
if say some data is "No", "Yes", this can be used in logistic
regression a normal, like    

glm.fit = glm(default~income+balance+student,data = Default,family = binomial)
here the student is dummy variable.

#standard deviation
sd

#confidence interval
t.test(Boston$medv)

#sequence or range of numbers
seq(1,10,by=0.2)

#random seed
set.seed(x)

#available colors
colors()

#lasso
library(glmnet)
grid = 10^seq(10,-2,length = n)
lasso.mod = glmnet(xdat[train,],y[train],alpha=1,lambda=grid)
cv.out = cv.glmnet(xdat[train,],y[train],alpha=1)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod,s=bestlam,newx = xdat[test,])
mean.lasso = mean((lasso.pred-y.test)^2)

out.lasso = glmnet(xdat,y,alpha=1,lambda=grid)
lasso.coef = predict(out.lasso,type="coefficients",s=bestlam)[1:11,]


#matrix of data like data frame
xdat = model.matrix(y~.,fr)[,-1] # 

#best subset
library(leaps)
y.fit = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10)
res = summary(y.fit)
c = coef(y.fit,10)
a1 = which.min(res$cp)
a2 = which.min(res$bic)
a3 = which.max(res$adjr2)

#forward step selection
y.fit.fwd = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10,method="forward")
res.fwd = summary(y.fit.fwd)
c.fwd = coef(y.fit.fwd,10)
a1.fwd = which.min(res.fwd$cp)
a2.fwd = which.min(res.fwd$bic)
a3.fwd = which.max(res.fwd$adjr2)


#backward step selection
y.fit.bck = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10,method="backward")
res.bck = summary(y.fit.bck)
c.bck = coef(y.fit.bck,10)
a1.bck = which.min(res.bck$cp)
a2.bck = which.min(res.bck$bic)
a3.bck = which.max(res.bck$adjr2)





