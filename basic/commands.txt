#this is just a list of common commands for R

#number of rows
row(var) - tells the number of rows

#all columns except the first
college = college[,-1]

#read csv
acsv <- read.csv("some.csv")

#show csv
fix(acsv)

#repeat
rep(val,number)

#load a source file
source("filename")

#make classification numeric
factor(var) or as.factor(var)

#add new column to data.frame
college=data.frame(college,newcol)

#number of rows or columns of data.frame
nrow(data) or ncol(data)

#histogram
hist(var, breaks=x)

#split plot area into 2 by 2
par(mfrow=c(2,2))

#transpose matrix
t(x)

#clear variables in workspace
rm(list=ls())

#get working directory
getwd()

#set working directory
setwd()

#read table data (assume table is called Auto.data, treat question marks as missing data)
Auto=read.table("Auto.data",header=T, na.strings="?")

#get column names from table
names(Auto)

#show type
class(var)

#apply function over data.frame or list or vector - here range is used, could have used mean or sd (standard deviation for example)
sapply(Auto[,1:7],range)
apply(mat,2,mean)

#remove rows 10 to 85 for example
newAuto = Auto[-(10:85),]

#load library (in this example loading the MASS library
library(MASS)

#get subset - this example gets the subset of chas column that are equal to 1.
subset(Boston,chas == 1)

#median 
median

#uniform random numbers 
runif(100,-1,1)

#linear model fit
lm(y~x)
lm(y~x+I(x^2)) #this is quadratic term also

#plotting on same plot - type is point plot, second plot uses cross hairs
plot(x,y,type='p',col='red')
lines(x,yf1,type='p',pch=4,col='black')

#list files in directory
list.files(".") #this is for current directory

#use a large mark on plot cex
lines(mx,my,type='p',pch=4,cex=4,col='black')

#plot linear regression line fit
plot(c(0,50),c(0,200)).
abline(lm.fit)

#plot linear regression line fit against residuals and more
plot(lm.fit)

#plot scatter plot
pairs(Auto)

#normal distribution
rnorm

#add line to plot
abline(lm.fit,lwd=3,col=2) #using a linear regression fit
abline(-1,0.5,lwd=3,col=3) # using a intercept and slope

#confidence interval 
confint(lm.fit)

#length of data
length

#linear regression fit all
lm(y~.,data=Boston)

#linear regresssion fit all except age
lm(medv~.-age,data=Boston)

#make numeric to classification
Boston$chas = factor(Boston$chas,labels = c("N","Y"))

#correlation
cor(Boston[-c(1,4)]) # correlation ignoring columns 1 and 4

#polynomial regression
lm(crim ~ poly(zn, 3)) #this creates B0 + B1*X + B2*X^2 + B3 *X^3

#logistic regression
glm.fit = glm(dir_dat~Weekly$Lag1+Weekly$Lag2+Weekly$Lag3+Weekly$Lag4+Weekly$Lag5+Weekly$Volume,family=binomial)

#replicate elements of a list
rep("Down",1250)

#compare objects
all.equal(a,b)
identical(a,b)

#create cross table (confusion matrix for example)
table(glm.pred,Direction)

#convert list to character array (flatten a list)
unlist

#convert from data.frame to numeric or character
Weekly["Lag2"][,] # this is same as Weekly$Lag2

#get variable args from function
some_func(...)
{
    dots = list(...)
}

#confusion matrix logisitic regression
train = (Weekly$Year < 2009)
Weekly.0910 = Weekly[!train, ]
glm.fit = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs = predict.glm(glm.fit,newdata= Weekly.0910, type = "response")
glm.pred = rep("Down", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Up"
Direction.0910 = Weekly$Direction[!train]
conf_mat = table(glm.pred, Direction.0910)
correct = ((conf_mat[1,1] + conf_mat[2,2])/sum(conf_mat)) * 100
error_rate = 100 - correct

#confusion matrix lda
lda.fit = lda(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
lda.pred = predict(lda.fit,Weekly.0910)
lda.class = lda.pred$class
conf_mat_lda = table(lda.class,Direction.0910)
correct_lda = (conf_mat_lda[1,1] + conf_mat_lda[2,2])/sum(conf_mat_lda) * 100
error_rate_lda = 100 - correct_lda

#confusion matrix with knn
lag2_train = as.matrix(Weekly$Lag2[train])
lag2_test = as.matrix(Weekly$Lag2[!train])
dir_train = Weekly$Direction[train]
dir_test = Weekly$Direction[!train]
knn.pred = knn(lag2_train,lag2_test,dir_train,k=1)
conf_mat_knn = table(knn.pred,dir_test)


#create matrix
as.matrix()

#error rate
mean(pred == direction)*100

#boxplot
boxplot(displacement~mpg,data=Auto,main="displacement vs mpg")

#for loop
for (i in 1:10000)
    store[i]=i^2

#sample data
sample(1:100,rep=TRUE) # take samples from the 1:100 data with replacement

#install a package
install.packages("scatterplot3d")

#dummy variable
if say some data is "No", "Yes", this can be used in logistic
regression a normal, like    

glm.fit = glm(default~income+balance+student,data = Default,family = binomial)
here the student is dummy variable.

#standard deviation
sd

#confidence interval
t.test(Boston$medv)

#sequence or range of numbers
seq(1,10,by=0.2)

#random seed
set.seed(x)

#available colors
colors()

#lasso
library(glmnet)
grid = 10^seq(10,-2,length = n)
lasso.mod = glmnet(xdat[train,],y[train],alpha=1,lambda=grid)
cv.out = cv.glmnet(xdat[train,],y[train],alpha=1)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod,s=bestlam,newx = xdat[test,])
mean.lasso = mean((lasso.pred-y.test)^2)
out.lasso = glmnet(xdat,y,alpha=1,lambda=grid)
lasso.coef = predict(out.lasso,type="coefficients",s=bestlam)[1:11,]

#another lasso
lasso.fit = cv.glmnet(x_train,y_train,alpha=1,lambda=grid,thresh=1e-12)
bestlam1 = lasso.fit$lambda.min
lasso.pred = predict(lasso.fit,s=bestlam1,newx = x_test)
mean_sq_lasso = mean((lasso.pred-y_test)^2)
x_full = model.matrix(Apps~.,data=college[,-1])
y_full = college$Apps
lasso.full.fit = glmnet(x_full,y_full,alpha=0,lambda=grid,thresh=1e-12)
lasso.coef = predict(lasso.full.fit,type="coefficients",s=bestlam1)



#matrix of data like data frame
xdat = model.matrix(y~.,fr)[,-1] # 

#best subset

library(leaps)
y.fit = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10)
res = summary(y.fit)
c = coef(y.fit,10)
a1 = which.min(res$cp)
a2 = which.min(res$bic)
a3 = which.max(res$adjr2)

#forward step selection
y.fit.fwd = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10,method="forward")
res.fwd = summary(y.fit.fwd)
c.fwd = coef(y.fit.fwd,10)
a1.fwd = which.min(res.fwd$cp)
a2.fwd = which.min(res.fwd$bic)
a3.fwd = which.max(res.fwd$adjr2)


#backward step selection
y.fit.bck = regsubsets(y~poly(x,10,raw=T),data=fr,nvmax=10,method="backward")
res.bck = summary(y.fit.bck)
c.bck = coef(y.fit.bck,10)
a1.bck = which.min(res.bck$cp)
a2.bck = which.min(res.bck$bic)
a3.bck = which.max(res.bck$adjr2)

#split data into test and training
train = sample(length(dat), length(dat)/2)
test = (-train)
train_dat = dat[train]
test_dat = dat[test]

#mean sq error
mean_sq_lm = mean((pred - college_test$Apps)^2)

#ridge regression
library(glmnet)
x_train = model.matrix(Apps~.,data=college_train[,-1])
y_train = college_train$Apps
x_test = model.matrix(Apps~.,data=college_test[,-1])
y_test = college_test$Apps
grid = 10^seq(10,-2,length = 100)
ridge.fit = cv.glmnet(x_train,y_train,alpha=0,lambda=grid,thresh=1e-12)
bestlam = ridge.fit$lambda.min
ridge.pred = predict(ridge.fit,s=bestlam,newx = x_test)
mean_sq_ridge = mean((ridge.pred-y_test)^2)

#another linear model fit
#fit a linear model
dft = college_train[,-1]
lm.fit = lm(Apps~.,data=dft)
pred = predict(lm.fit,college_test[,-1])
#get mean sq error
mean_sq_lm = mean((pred - college_test$Apps)^2)

#now try the PCR
library(pls)
pcr.fit = pcr(Apps~.,data=college[,-1],subset=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type="MSEP") # this gives  about 10 good choice
pcr.pred = predict(pcr.fit,college_test[,-1],ncomp=17)
mean_sq_pcr = mean((pcr.pred - y_test)^2)

#get R2 (R squared) from the data
mx = mean(test_dat)
mx_sq = ((test_dat- mx)^2)
R2_lm = ((test_dat - lm.pred)^2) / mx_sq

#bar plot
barplot(c(R2_lm,R2_ridge,R2_lasso,R2_PCR,R2_PLS),col="blue",names.arg=c("R2_lm","R2_ridge","R2_lasso","R2_PCR","R2_PLS"), main="Test R sq")
 
#create data set (n by p)
n=1000
p=20
x = rnorm(n,0,1)
e = rnorm(n,0,0.01)
xem = as.matrix(x) # 1 by 1000
B = rnorm(p) #20 by 1 
B[4] = 0
B[11] = 0
B[14]=0
B[19]=0

y = xem %*% B + e # matrix multiply

# column names
x_xcols = colnames(x,do.NULL = FALSE,prefix = "x.")

#get training MSE for subsets
val.errors =rep(NA,p)
x_cols = colnames(xem,do.NULL = FALSE,prefix = "X")
for(i in 1:p)
{
    coefi = coef(fit.sub,id=i)
  #  pred = as.matrix(train_set_x[,x_cols %in% names(coefi)])
    pred = as.matrix(train_set_x[,x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in% x_cols]
    print(pred)
    val.errors[i] = mean((train_set_y - pred)^2)
}
plot(val.errors,ylab= "Training MSE", pch=19,type="b")

#get index at which max or min
which.min(data)
which.max(data)

#sample data into k folds
folds = sample(rep(1:k,length=100))
for(i = 1:k)
 {
     dat = Boston[folds != i,]
 }

#predict for regsubsets with call below
predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    print(coefi)
#this is the returned value
   # print(dim(mat[,names(coefi)] %*% coefi))
#note mat here can be 50 by n. coefi is n by 1 (can be written as 1 by n)
#result is 50 by 1
    mat[, names(coefi)] %*% coefi
}
pred = predict(fit_sub,Boston[folds == i,],id = j)

# model matrix - can expand factors to dummy variables. here we remove the first column (everything except crim)
model.matrix(crim~.-1,data=Boston)

#another lasso with mse rmse
library(glmnet)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.lasso = cv.glmnet(x, y, type.measure = "mse")
plot(cv.lasso)
#perform the rmse with the lasso
#get rmse
rmse.lasso = sqrt(cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se])

#fit data with poly and use k fold cross validation
library(ISLR)
set.seed(3)
p=10
cv.error = rep(NA,10)
for (i in 1:p)
{
    glm.fit = glm(wage~poly(age,i), data= Wage)
    cv.error[i] = cv.glm(Wage,glm.fit,K=p)$delta[1]
}

plot(1:p,cv.error,xlab="degree",ylab="CV error",type="l",pch=20,lwd=2,ylim=c(1590,1700))
min.point = min(cv.error)
sd.points = sd(cv.error)
abline(h=min.point+0.2 * sd.points, col="red",lty="dashed")
abline(h=min.point-0.2 * sd.points, col="blue",lty="dashed")

#nice plot showing predict fit with standard error bands
agelims = range(Wage$age)
age.grid = seq(from=agelims[1],to=agelims[2])
lm.fit = lm(wage~poly(age,3),data=Wage)
preds = predict(lm.fit,newdata=list(age=age.grid),se=TRUE)
se.bands = cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
par(mfrow=c(1,1),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(Wage$age,Wage$wage,xlim=agelims,cex=0.5,col="darkgrey")
title("degree 3 poly",outer=T)
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="red",lty=3)

#predict value using binomial (value > 250)  
fit.step = glm(I(wage > 250)~poly(age,3),data=Wage,family=binomial)
preds.step = predict(fit.step,newdata=list(age=age.grid),se=T)
pfit = exp(preds.step$fit) / (1 + exp(preds.step$fit))
se.bands.logit = cbind(preds.step$fit+2*preds.step$se.fit, preds.step$fit-2*preds.step$se.fit)
se.bands = exp(se.bands.logit)/(1+ exp(se.bands.logit))

plot(Wage$age,I(Wage$wage > 250), xlim=agelims,type="n", ylim=c(0,.2))
points(jitter(Wage$age), I((Wage$wage > 250) / 5), cex= 0.5,pch="|", col="darkgrey")
lines(age.grid,pfit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="red",lty=3)

#cut - split / section / part data into regions / groups
table(cut(Wage$age,4))

#fit a step function
#use a step function use k fold cross validation to get best number of cut points
error.steps = rep(NA,9)
for (i in 2:10)
{
    Wage$age.cut = cut(Wage$age,i)
    lm.fit = glm(wage~age.cut,data=Wage)
    error.steps[i-1] = cv.glm(Wage,lm.fit,K=10)$delta[2]
}
plot(2:10,error.steps,xlab="Number of cuts",ylab="CV error",type="l",pch=20,lwd=2)

min_step = which.min(error.steps) + 1
lm.step = glm(wage~cut(age,min_step),data=Wage)
lm.step.pred = predict(lm.step,data.frame(age=age.grid))
plot(wage~age,data=Wage,col="darkgrey")
lines(age.grid,lm.step.pred,col="red",lwd=2)
